# -*- coding: utf-8 -*-
"""PR3_Averyanov_P.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nUYBKj-vTz_EeHy4GlbjVbhsed6vQDzL

Практическая работа

Обнаружение вредоносного программного обеспечения (ВПО)

Обучить модель машинного обучения для обнаружения ВПО на основе открытых наборов данных
"""

# Импорт необходимых библиотек
import numpy as np # для линейной алгебры
import pandas as pd # для обработки данных, работы с CSV файлами
import pickle # для сериализации объектов Python
import seaborn as sns # для визуализации данных
import matplotlib.pyplot as plt # для построения графиков
from sklearn.ensemble import RandomForestClassifier # импорт алгоритма случайного леса
from sklearn.model_selection import train_test_split # для разделения данных на обучающую и тестовую выборки
from sklearn.metrics import classification_report, confusion_matrix # для оценки производительности модели

import os # модуль для работы с файловой системой

# Поиск файлов в директории '/content/input' и вывод их путей
for dirname, _, filenames in os.walk('/content/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Установка стиля визуализации графиков на 'ggplot'
plt.style.use('ggplot')

# Комментарий о возможности записи временных файлов и выходных данных
# Вы можете записать до 20 ГБ в текущую директорию (/kaggle/working/), которая сохраняется при создании версии с помощью "Save & Run All".
# Вы также можете записывать временные файлы в /kaggle/temp/, но они не будут сохранены за пределами текущей сессии.

data = pd.read_csv('dataset_malwares.csv')

data.head()

data.info()

used_data = data.drop(['Name', 'Machine', 'TimeDateStamp', 'Malware'], axis=1)

"""Данный код использует библиотеку seaborn для визуализации данных, создавая график, который показывает количество наблюдений для каждой категории в столбце Malware из DataFrame data"""

# Установка размера графика
plt.figure(figsize=(8, 6))

# Создание графика с помощью функции countplot из библиотеки seaborn
ax = sns.countplot(data['Malware'])

"""Для создания гистограммы, которая отображает распределение значений в указанном столбце Malware из DataFrame data. Гистограмма позволяет визуализировать, как часто встречаются различные значения в этом столбце, что полезно для анализа данных."""

data.hist(column='Malware')

data['Malware']

"""Данный код создает распределения (графики плотности) для нескольких признаков (features) в наборе данных, разделяя их по категориям "вредоносные" (Malware) и "безвредные" (Benign)."""

# Список признаков, для которых будут построены графики распределения
features = ['MajorSubsystemVersion', 'MajorLinkerVersion', 'SizeOfCode', 'SizeOfImage',
            'SizeOfHeaders', 'SizeOfInitializedData', 'SizeOfUninitializedData',
            'SizeOfStackReserve', 'SizeOfHeapReserve', 'NumberOfSymbols', 'SectionMaxChar']

# Инициализация переменной для индекса подграфиков
i = 1

# Цикл по каждому признаку из списка features
for feature in features:
    # Создание новой фигуры с заданными размерами
    plt.figure(figsize=(10, 15))

    # Создание подграфика для вредоносных образцов (Malware)
    ax1 = plt.subplot(len(features), 2, i)
    sns.distplot(data[data['Malware'] == 1][feature], ax=ax1, kde_kws={'bw': 0.1})
    ax1.set_title(f'Malware', fontsize=10)

    # Создание подграфика для безвредных образцов (Benign)
    ax2 = plt.subplot(len(features), 2, i + 1)
    sns.distplot(data[data['Malware'] == 0][feature], ax=ax2, kde_kws={'bw': 0.1})
    ax2.set_title(f'Benign', fontsize=10)

    # Увеличение индекса для следующей пары подграфиков
    i = i + 2

"""Для разделения набора данных на обучающую и тестовую выборки"""

# Импортируем функцию train_test_split из библиотеки sklearn
from sklearn.model_selection import train_test_split

# Разделяем данные на обучающую и тестовую выборки
# used_data - это набор признаков, а data['Malware'] - целевая переменная
X_train, X_test, y_train, y_test = train_test_split(
    used_data,          # Признаки для обучения
    data['Malware'],    # Целевая переменная (метки классов)
    test_size=0.2,      # 20% данных отводится для тестирования
    random_state=0      # Фиксированное значение для воспроизводимости
)

"""Используется для вывода на экран количества признаков (features), которые были использованы в обучающей выборке X_train"""

print(f'Number of used features is {X_train.shape[1]}')

"""Building the model

Создает экземпляр классификатора случайного леса (Random Forest Classifier) с заданными параметрами. Затем rfc.fit(X_train, y_train) обучает модель на обучающих данных.

n_estimators=100: Указывает количество деревьев в лесу. Чем больше деревьев, тем более стабильные и точные будут предсказания, но это также увеличивает время обучения.

random_state=0: Устанавливает фиксированное значение для генератора случайных чисел, что позволяет воспроизводить результаты при каждом запуске кода.

oob_score=True: Включает использование "вне пакета" (out-of-bag) для оценки обобщающей способности модели. Это позволяет использовать данные, которые не были выбраны для обучения каждого дерева, для оценки производительности модели.

max_depth=16: Устанавливает максимальную глубину каждого дерева. Ограничение глубины помогает предотвратить переобучение, так как деревья не будут слишком сложными.
"""

# Импортируем RandomForestClassifier из библиотеки sklearn
from sklearn.ensemble import RandomForestClassifier

# Создаем экземпляр классификатора случайного леса с заданными параметрами
rfc = RandomForestClassifier(
    n_estimators=100,      # Количество деревьев в лесу
    random_state=0,        # Фиксированное значение для воспроизводимости
    oob_score=True,        # Использовать вне пакета для оценки
    max_depth=16           # Максимальная глубина деревьев
)

# Обучаем модель на обучающих данных
rfc.fit(X_train, y_train)  # X_train - признаки, y_train - целевая переменная

"""Используется для получения предсказаний модели случайного леса (Random Forest) на тестовой выборке X_test"""

# Получаем предсказания модели случайного леса для тестовой выборки X_test
# y_pred будет содержать предсказанные классы для каждого образца в X_test
y_pred = rfc.predict(X_test)

"""Classification report

Точность (Precision): Доля правильно предсказанных положительных наблюдений к общему числу предсказанных положительных наблюдений.

Полнота (Recall): Доля правильно предсказанных положительных наблюдений к общему числу фактических положительных наблюдений.

F1-мера: Гармоническое среднее между точностью и полнотой, которое учитывает как ложные положительные, так и ложные отрицательные результаты.

Поддержка (Support): Количество истинных экземпляров для каждого класса.
"""

# Импортируем функцию classification_report из библиотеки sklearn
from sklearn.metrics import classification_report

# Выводим отчет о классификации, сравнивая истинные метки y_test с предсказанными метками y_pred
# target_names задает имена классов для более понятного отображения в отчете
print(classification_report(y_test, y_pred, target_names=['Benign', 'Malware']))

"""Confusion matrix

Используется для визуализации матрицы ошибок (confusion matrix) с помощью тепловой карты
"""

# Импортируем необходимые библиотеки
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Создаем тепловую карту для визуализации матрицы ошибок
# y_pred - предсказанные метки, y_test - истинные метки
ax = sns.heatmap(
    confusion_matrix(y_pred, y_test),  # Генерация матрицы ошибок
    annot=True,                         # Отображение значений в ячейках
    fmt="d",                            # Формат значений как целые числа
    cmap=plt.cm.Blues,                 # Цветовая карта
    cbar=False                          # Отключение цветовой шкалы
)

# Устанавливаем метки для осей
ax.set_xlabel('Predicted labels')      # Метка для оси X
ax.set_ylabel('True labels')           # Метка для оси Y

"""Для сохранения обученной модели случайного леса (Random Forest) в файл. Это позволяет сохранить состояние модели, чтобы ее можно было загрузить и использовать позже без необходимости повторного обучения"""

# Импортируем библиотеку pickle для сериализации объектов
import pickle

# Задаем имя файла для сохранения модели
pkl_filename = "rf_model.pkl"

# Открываем файл в режиме записи в бинарном формате
with open(pkl_filename, 'wb') as file:
    # Сохраняем обученную модель случайного леса в файл
    pickle.dump(rfc, file)  # rfc - это обученная модель

"""Features Importance

Для вычисления и визуализации важности признаков (feature importance) модели случайного леса (Random Forest). Важность признаков показывает, насколько каждый признак (или переменная) влияет на предсказания модели
"""

# Извлекаем важность признаков из обученной модели случайного леса
importance = rfc.feature_importances_

# Создаем словарь, связывающий имена признаков с их значениями важности
importance_dict = {used_data.columns.values[i]: importance[i] for i in range(len(importance))}

# Сортируем словарь по значениям важности в порядке возрастания
sorted_dict = {k: v for k, v in sorted(importance_dict.items(), key=lambda item: item[1])}

# Создаем новую фигуру для графика
plt.figure(figsize=(10, 20))

# Создаем столбчатую диаграмму для визуализации важности признаков
sns.barplot(y=list(sorted_dict.keys())[::-1], x=list(sorted_dict.values())[::-1], palette='mako')

# Устанавливаем заголовок для графика
plt.title('Features importance')

"""Вывод

Модель случайного леса дает очень хорошие результаты без какой-либо предварительной обработки данных. Результат хорош, несмотря на то, что данные несбалансированы. Итак, я обнаружил, что нам не нужно использовать какие-либо методы для восстановления баланса. Масштабирование не является обязательным, модель случайного леса - это рекурсивная модель секционирования, зависящая от разделения данных, поскольку она работает над разделением значений объектов, а не производит вычисления на их основе.
"""